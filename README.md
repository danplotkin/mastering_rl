# Mastering Reinforcement Learning: Theory, Math, and Python

## Course Modules:

> 1. [Part I: Theory and Math](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb)
<details>
   <summary> Show more </summary>
>[Introduction to Reinforcement Learning](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=9-9rUEY2mo9a)
   
>>[Reinforcement Learning vs Supervised and Unsupervised Learning](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=akvqtK_vbUJF)

>>[Use Cases for Reinforcement Learning](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=BWxTKKSebhwu)

>>[Markov Decision Processes (MDP)](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=fcuasjvunYGv)

>>>[A. Markov Property](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=DLv66adWna4G)

>>>[B. Agent-Environment Interaction in MDPs](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=JgcljOBwndhn)

>>>[C. State-Action Representation in MDPs.](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=L0nxi9sfngNu)

>>>[D. Mars Rover Example Introduction](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=HkgIA0ItdJaM)

>>>[E. MDP Trajectory](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=JaxB5Uxwnif2)

>>>[F. Transition Probabilities](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=TUtM3bW1nmjJ)

>>>>[I. Transition Probabilities with Stochastic Environment (Mars Rover)](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=9wglnomfvLlP)

>>>[G. Expected Return](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=MiU34uW1nrLR)

>>>>[I. Example with Mars Rover](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=lG-R0F4l3KTE)

>>>[H. Policies](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=x1jrBqhWQN0s)

>>>[I. Value Functions](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=tlamPbi7Vrvj)

>>>[J. Representing MDP as a Tuple](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=K2g6_pReay6x)

>>[Policy Optimality](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=jG0-HB76hwPT)

>>>[A. Policy Improvement Theorem](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=_xrjl-zmpCwg)

>>>[B. Optimal State-Value Function](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=_F9mu0-W4p7o)

>>>[C. Optimal Action-Value Function](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=CR2qnP-SBYd8)

>>>[D. Bellman Optimality Equation for $Q^*$](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=yi3HoW-Jn5HL)

>>>[E. Deriving Optimal Policy](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=747OG0dPU7d4)

>>[Q-Learning](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=wepK9xsw3sZS)

>>>[A. Q-Value Table](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=3Ca3sxI3gHNv)

>>>>[I. Initialization](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=0tDTbVXsgLKK)

>>>[B. Exploration Vs Exploitation](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=EySyN_4OUgOR)

>>>>[I. Epsilon Greedy Strategy](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=4SgZyIjhZPpg)

>>>[C. Q-value Update with Q-Learning Algorithm](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=RSwVXzkCgcCW)

>>>[D. Mars Rover Q-Learning Example](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=Dh2CauP9Z0KW)

>>>>[Step 1. Q-Value Table Initialization](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=Dh2CauP9Z0KW)

>>>>[Step 2: Current State $s_4$](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=Dh2CauP9Z0KW)

>>>>[Step 3: Transition and Reward](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=Dh2CauP9Z0KW)

>>>>[Step 4: Q-Value Update](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=Dh2CauP9Z0KW)

>>>>[Step 5: Update Q Table:](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=Dh2CauP9Z0KW)

>>[Deep Q-Learning](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=c3iKjQMQnG49)

>>>[A. Deep Q-Networks (DQN)](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=KilZ4mPTzPSl)

>>>>[I. Policy Network Architecture](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=HaEfX4bvps8r)

>>>>[II. Loss Calculation](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=13Fgz5ZypFV9)

>>>>[III. Update Parameters](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=z77xcioF1zvf)

>>>[B. Experience Replay & Replay Memory](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=TsE6pvapqUEA)

>>>>[I. Replay Memory as a Tuple](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=DBEdMyjqu-d7)

>>>>[II. Randomly Sampling Replay Memory](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=_I2dEDDQvRKT)

>>>>[III. Training with Replay](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=llc3SZcZvMMR)

>>[Training a DQN](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=I7kN5qId108M)

>>>[A. Training Steps](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=AQOc87ehbvoY)

>>>>[I. Sample a Random Batch from Replay Memory.](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=rBmV4iBGjN1g)

>>>>[II. Preprocess the State](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=RtuMBKEEjP6y)

>>>>[III. Forward Propagation](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=U6o2_-EdkNjF)

>>>>[IV. Calculate Loss](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=Dsz5GRVPjh8W)

>>>>[V. Backpropagation & Gradient Descent](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=CSDluyVEvKXn)

>>>[B. Full Training Loop](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=SVRu7rydwThH)

>>>[C. Limitations of Standard DQNs](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=OPbovuUNgudF)

>>[Target Network](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=-5wpSYDdggXg)

>>>[A. Initialization](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=UQpvGiNMoW3c)

>>>[B. Soft Update](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=hMazPWK0pLLm)

>>>[C. Updated Training Process](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=yE4em57vpuIR)

>[Next Steps](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb#scrollTo=nm5hJe3oo-DL)

</details>

> 2. [Part II: Implementing Reinforcement Learning in Python](https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part2.ipynb)

## About
Welcome to my course *Mastering Reinforcement Learning: Theory, Math, and Python*! This course will be split up into two parts:
* Part I: Theory and Math
* Part II: Implementing Reinforcement Learning in Python 

By the end of this course, you will have a strong understanding on what reinforcement learning is, the mathematical foundation of reiforcement learning, Markov Decision Process (MDP), Q-learning, Deep-Q Learning, and how you can implement all of these skills using python in a real world project!

## Prerequisites

Before starting the Reinforcement Learning course, it's recommended to have the following skills and knowledge:

1. **Python Programming Skills:** Object-oriented programing skills in Python.

2. **Foundational Mathematics:** Understanding of basic calculus, linear algebra, and probability theory will be helpful for comprehending the algorithms and concepts.

3. **Machine Learning Basics:** Familiarity with fundamental concepts in machine learning, such as supervised and unsupervised learning, will provide a good foundation.

4. **Understanding of Algorithms and Data Structures:** Basic knowledge of algorithms and data structures will aid in comprehending the logic and implementation of RL algorithms.

5. **Familiarity with Markov Processes:** Having a basic understanding of Markov processes and probability will be beneficial for understanding Markov Decision Processes (MDPs).

6. **Neural Networks:** Basic understanding of neural networks and their functioning will be useful.
   
7. **Experience with Frameworks:** Familiarity with machine learning/deep learning frameworks like TensorFlow or PyTorch can be beneficial for implementing algorithms. We will be using PyTorch.
