{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+ys6Ul9uBm/zvYcrmnRNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danplotkin/mastering_rl/blob/main/mastering_rl_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mastering Reinforcement Learning: Theory, Math, and Python\n",
        ">  _**Part I: Theory and Math**_\n",
        "\n",
        "> *Author:  Daniel M. Plotkin*\n",
        "\n",
        "> *Author Email: danmplotkin@gmail.com*\n",
        "\n",
        "> *Last Updated: 1/7/2024*\n",
        "\n",
        "> **[Click to Access Course Homepage](https://github.com/danplotkin/mastering_rl?tab=readme-ov-file#understanding-reinforcement-learning)**"
      ],
      "metadata": {
        "id": "J6BUF7yMmZWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents"
      ],
      "metadata": {
        "id": "13HZmbQhejVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Introduction to RL](#scrollTo=9-9rUEY2mo9a)\n",
        "\n",
        ">[Reinforcement Learning vs Supervised and Unsupervised Learning](#scrollTo=akvqtK_vbUJF)\n",
        "\n",
        ">[Use Cases for Reinforcement Learning](#scrollTo=BWxTKKSebhwu)\n",
        "\n",
        ">[Markov Decision Processes (MDP)](#scrollTo=fcuasjvunYGv)\n",
        "\n",
        ">>[A. Markov Property](#scrollTo=DLv66adWna4G)\n",
        "\n",
        ">>[B. Agent-Environment Interaction in MDPs](#scrollTo=JgcljOBwndhn)\n",
        "\n",
        ">>[C. State-Action Representation in MDPs.](#scrollTo=L0nxi9sfngNu)\n",
        "\n",
        ">>[D. Mars Rover Example Introduction](#scrollTo=HkgIA0ItdJaM)\n",
        "\n",
        ">>[E. MDP Trajectory](#scrollTo=JaxB5Uxwnif2)\n",
        "\n",
        ">>[F. Transition Probabilities](#scrollTo=TUtM3bW1nmjJ)\n",
        "\n",
        ">>>[I. Transition Probabilities with Stochastic Environment (Mars Rover)](#scrollTo=9wglnomfvLlP)\n",
        "\n",
        ">>[G. Expected Return](#scrollTo=MiU34uW1nrLR)\n",
        "\n",
        ">>>[I. Example with Mars Rover:](#scrollTo=lG-R0F4l3KTE)\n",
        "\n",
        ">>>>[Calculating Expected Return:](#scrollTo=lG-R0F4l3KTE)\n",
        "\n",
        ">>>>[Calculation:](#scrollTo=lG-R0F4l3KTE)\n",
        "\n",
        ">>>>[Interpretation:](#scrollTo=lG-R0F4l3KTE)\n",
        "\n",
        ">>[H. Policies](#scrollTo=x1jrBqhWQN0s)\n",
        "\n",
        ">>[I. Value Functions](#scrollTo=tlamPbi7Vrvj)\n",
        "\n",
        ">>[J. Representing MDP as a Tuple](#scrollTo=K2g6_pReay6x)\n",
        "\n",
        ">[Policy Optimality](#scrollTo=jG0-HB76hwPT)\n",
        "\n",
        ">>[A. Policy Improvement Theorem](#scrollTo=_xrjl-zmpCwg)\n",
        "\n",
        ">>[B. Optimal State-Value Function](#scrollTo=_F9mu0-W4p7o)\n",
        "\n",
        ">>[C. Optimal Action-Value Function](#scrollTo=CR2qnP-SBYd8)\n",
        "\n",
        ">>[D. Bellman Optimality Equation for $Q^*$](#scrollTo=yi3HoW-Jn5HL)\n",
        "\n",
        ">>[E. Deriving Optimal Policy](#scrollTo=747OG0dPU7d4)\n",
        "\n",
        ">[Q-Learning](#scrollTo=wepK9xsw3sZS)\n",
        "\n",
        ">>[A. Q-Value Table](#scrollTo=3Ca3sxI3gHNv)\n",
        "\n",
        ">>>[I. Initialization](#scrollTo=0tDTbVXsgLKK)\n",
        "\n",
        ">>[B. Exploration Vs Exploitation](#scrollTo=EySyN_4OUgOR)\n",
        "\n",
        ">>>[I. Epsilon Greedy Strategy](#scrollTo=4SgZyIjhZPpg)\n",
        "\n",
        ">>[C. Q-value Update with Q-Learning Algorithm](#scrollTo=RSwVXzkCgcCW)\n",
        "\n",
        ">>[D. Mars Rover Q-Learning Example](#scrollTo=Dh2CauP9Z0KW)\n",
        "\n",
        ">>>[Step 1. Q-Value Table Initialization](#scrollTo=Dh2CauP9Z0KW)\n",
        "\n",
        ">>>[Step 2: Current State $s_4$](#scrollTo=Dh2CauP9Z0KW)\n",
        "\n",
        ">>>[Step 3: Transition and Reward](#scrollTo=Dh2CauP9Z0KW)\n",
        "\n",
        ">>>[Step 4: Q-Value Update](#scrollTo=Dh2CauP9Z0KW)\n",
        "\n",
        ">>>[Step 5: Update Q Table:](#scrollTo=Dh2CauP9Z0KW)\n",
        "\n",
        ">[Deep Q-Learning](#scrollTo=c3iKjQMQnG49)\n",
        "\n",
        ">>[A. Deep Q-Networks (DQN)](#scrollTo=KilZ4mPTzPSl)\n",
        "\n",
        ">>>[I. Policy Network Architecture](#scrollTo=HaEfX4bvps8r)\n",
        "\n",
        ">>>[II. Loss Calculation](#scrollTo=13Fgz5ZypFV9)\n",
        "\n",
        ">>>[III. Update Parameters](#scrollTo=z77xcioF1zvf)\n",
        "\n",
        ">>[B. Experience Replay & Replay Memory](#scrollTo=TsE6pvapqUEA)\n",
        "\n",
        ">>>[I. Replay Memory as a Tuple](#scrollTo=DBEdMyjqu-d7)\n",
        "\n",
        ">>>[II. Randomly Sampling Replay Memory](#scrollTo=_I2dEDDQvRKT)\n",
        "\n",
        ">>>[III. Training with Replay](#scrollTo=llc3SZcZvMMR)\n",
        "\n",
        ">[Training a DQN using Policy Network](#scrollTo=I7kN5qId108M)\n",
        "\n",
        ">>[A. Training Steps](#scrollTo=AQOc87ehbvoY)\n",
        "\n",
        ">>>[I. Sample a Random Batch from Replay Memory.](#scrollTo=rBmV4iBGjN1g)\n",
        "\n",
        ">>>[II. Preprocess the State](#scrollTo=RtuMBKEEjP6y)\n",
        "\n",
        ">>>[III. Forward Propagation](#scrollTo=U6o2_-EdkNjF)\n",
        "\n",
        ">>>[IV. Calculate Loss](#scrollTo=Dsz5GRVPjh8W)\n",
        "\n",
        ">>>[V. Backpropagation & Gradient Descent](#scrollTo=CSDluyVEvKXn)\n",
        "\n",
        ">>[B. Full Training Loop](#scrollTo=SVRu7rydwThH)\n",
        "\n",
        ">>[C. Limitations of Standard DQNs](#scrollTo=OPbovuUNgudF)\n",
        "\n",
        ">[Target Network](#scrollTo=-5wpSYDdggXg)\n",
        "\n",
        ">>[A. Initialization](#scrollTo=UQpvGiNMoW3c)\n",
        "\n",
        ">>[B. Periodic Update](#scrollTo=hMazPWK0pLLm)\n",
        "\n",
        ">>[C. Updated Training Process](#scrollTo=yE4em57vpuIR)\n",
        "\n",
        ">[Next Steps](#scrollTo=nm5hJe3oo-DL)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "63MaJmDI46hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to RL\n",
        "\n",
        "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to achieve a specific goal. The agent learns through trial and error, receiving feedback in the form of rewards or penalties based on its actions. The goal is for the agent to learn the optimal strategy or policy that maximizes cumulative rewards over time."
      ],
      "metadata": {
        "id": "9-9rUEY2mo9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning vs Supervised and Unsupervised Learning\n",
        "\n",
        "Before diving into reinforcement learning, it is important to understand how this paradigm differs from supervised and unsupervised learning methods.\n",
        "\n",
        "1) Feedback Mechanism:\n",
        "\n",
        "- **Supervised Learning:** Learns from labeled data with explicit output guidance.\n",
        "- **Unsupervised Learning:** Deals with unlabeled data, seeking hidden patterns without output labels.\n",
        "- **Reinforcement Learning:** Learns through interaction, receiving feedback as rewards or penalties based on actions taken.\n",
        "\n",
        "2) Goal-Oriented Learning:\n",
        "\n",
        "- **Supervised/Unsupervised Learning:** Focuses on pattern recognition or data-driven tasks without explicit goals.\n",
        "- **Reinforcement Learning:** Goal-oriented, learning to maximize cumulative rewards through actions in an environment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akvqtK_vbUJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Cases for Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning has applications in many domains where decision-making occurs through interaction with an environment. Some prominent use cases for RL include:\n",
        "\n",
        "1. **Game Playing:** RL has seen great success in mastering complex games. AlphaGo, developed by DeepMind, famously defeated human Go champions, showcasing RL's capability to learn complex strategies.\n",
        "\n",
        "2. **Robotics:** RL enables robots to learn tasks by trial and error. It's used for robotic control, manipulation, and autonomous systems, allowing robots to adapt to diverse environments.\n",
        "\n",
        "3. **Autonomous Vehicles:** RL plays a crucial role in training self-driving cars to make decisions in real-time scenarios, such as lane changing, traffic light recognition, and pedestrian avoidance.\n",
        "\n",
        "4. **Finance:** RL is employed in algorithmic trading to optimize trading strategies by learning from market data. It's also used for portfolio management and risk assessment.\n",
        "\n",
        "5. **Healthcare:** In personalized treatment plans and drug discovery, RL helps in optimizing treatments and understanding molecular interactions for drug development.\n",
        "\n",
        "6. **Recommendation Systems:** RL is used to improve recommendation algorithms by learning from user interactions to suggest personalized content, products, or services.\n",
        "\n",
        "7. **Resource Management:** In energy systems, RL can optimize power grid management, scheduling, and resource allocation for maximum efficiency.\n",
        "\n",
        "8. **Marketing and Advertising:** RL assists in optimizing marketing strategies, ad placement, and pricing to maximize returns based on user engagement and response.\n",
        "\n",
        "9. **Simulated Environments:** RL is utilized to train agents in simulated environments for various purposes, including training virtual assistants, simulating industrial processes, and optimizing logistics.\n",
        "\n",
        "10. **Education:** RL can be applied in adaptive learning environments, where systems adjust content based on student performance, optimizing the learning experience.\n",
        "\n"
      ],
      "metadata": {
        "id": "BWxTKKSebhwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Processes (MDP)\n",
        "\n",
        "MDPs are the foundational mathematical framework for understanding and solving RL problems."
      ],
      "metadata": {
        "id": "fcuasjvunYGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Markov Property\n",
        "\n",
        "MDPs utilize something called the Markov property, which states that a process is to be \"Markovian\" or possess the \"Markov property\" if its future behavior or state depends only on its current state and not on the sequence of events or states that preceded it."
      ],
      "metadata": {
        "id": "DLv66adWna4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Agent-Environment Interaction in MDPs\n",
        "\n",
        "In a MDP there is an that **agent** interacts with the **environment** that it is placed in. These actions occur in a sequential order. At each time step $t$, the agent will get a certain representation of the current **state** $S_t$ it is in. Based on this representation, the agent will take an **action** $A_t$, which shifts the environment into a new state $S_{t+1}$. The agent is given a **reward** $R_{t+1}$ based on the result of the previous action. The image below from [deepai.org](https://deepai.org/machine-learning-glossary-and-terms/markov-model) provides a visualization of the MDP:\n",
        "![image](https://images.deepai.org/django-summernote/2019-03-19/6e934c07-9ac4-4321-8803-687049be4796.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "JgcljOBwndhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. State-Action Representation in MDPs.\n",
        "\n",
        "At each time step $t$, the agent gets a representation of the environments state $S_t \\in \\mathbf{S}$. Based on this representation, the agent takes an action $A_t \\in \\mathbf{A}$. This creates something known as the state-action pair ($S_t, A_t$). After an action is taken, the time step gets incremented as $t+1$, thus putting the agent into $S_{t+1}$. Based on the state-action pair at $t$, a reward $R_{t+1} \\in \\mathbf{R}$ is calculated based on the environment's reward function."
      ],
      "metadata": {
        "id": "L0nxi9sfngNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Mars Rover Example Introduction\n",
        "\n",
        "In this course, we will use a Mars rover scenario to illustrate and improve our understanding of key topics related to MDPs. This example is from Professor Brunskill at Stanford University. The Mars rover serves as a practical and relatable example, allowing us to explore the concepts in a tangible context.\n",
        "\n",
        "<img src=\"https://www.techexplorist.com/wp-content/uploads/2020/03/Mars-2020_rover.jpg\" alt=\"Alt text\" width=\"500\" height=\"300\" />\n",
        "\n",
        "\n",
        "**State Space:**\n",
        "\n",
        "The rover can be in one of seven states denoted as $s_1$, $s_2$, $s_3$, $s_4$, $s_5$, $s_6$, and $s_7$. Each state represents a specific location or condition of the rover on Mars. The rover starts in $s_4$.\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "The rover can take one of three actions at each time step $t$:\n",
        "\n",
        "$a_1$: Move left.\n",
        "\n",
        "$a_2$: Stay in same spot.\n",
        "\n",
        "$a_3$: Move right.\n",
        "\n",
        "**Reward System:**\n",
        "\n",
        "The table below outlines the rewards associated with the rover's transitions between states. The \"Reward\" column indicates the immediate reward received when the rover moves from the previous state to the next state after taking a specific action.\n",
        "\n",
        "| States         | $s_1$ | $s_2$ | $s_3$ | $s_4$ | $s_5$ | $s_6$ | $s_7$ |\n",
        "|----------------|-------|-------|-------|-------|-------|-------|-------|\n",
        "| Reward         | 0     | 1     | 0     | 0     | 0     | 10    |  0    |\n",
        "| Current State  |       |       |       | *     |       |       |       |\n",
        "\n",
        "**Terminal States**:\n",
        "\n",
        "A terminal state represents the end of the \"episode\", or the end of the specific simulation iteration. This will represent when our rover falls off our platform. The two terminal state-action pairs are:\n",
        "\n",
        "1. $(s_1, a_1)$\n",
        "2.  $(s_7, a_3)$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HkgIA0ItdJaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E. MDP Trajectory\n",
        "\n",
        "An MDP trajectory refers to the sequence of states, actions, and rewards experienced by an agent as it navigates through an environment following a specific policy. It's essentially the path or journey taken by the agent from an initial state to a terminal state, considering the actions it takes at each step and the rewards it receives. A trajectory can look like this:\n",
        "\n",
        "$$S_0, A_0, R_1, S_1, A_1, R_2, ...$$\n",
        "\n",
        "For example, with are mars rover example:\n",
        "\n",
        "| States         | $s_1$ | $s_2$ | $s_3$ | $s_4$ | $s_5$ | $s_6$ | $s_7$ |\n",
        "|----------------|-------|-------|-------|-------|-------|-------|-------|\n",
        "| Reward         | 0     | 1     | 0     | 0     | 0     | 10    |  0    |\n",
        "| Current State  |       |       |       | *     |       |       |       |\n",
        "\n",
        "we can have a trajectory like this:\n",
        "\n",
        "$$\n",
        "\\text{trajectory} = (s_4, a_3, 0, s_5, a_3, 10, s_6, a_3, 0, s_7, a_3, \\text{terminal})\n",
        "$$\n",
        "\n",
        "Here our trajectory shows us starting in $s_4$, and moving right until we fall off the platform and terminate the episode. We use this trajectory to view the history of our episode and what action were taken to get into each state of the episode.\n"
      ],
      "metadata": {
        "id": "JaxB5Uxwnif2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F. Transition Probabilities\n",
        "\n",
        "Transition probabilities in a Markov Decision Process (MDP) describe the likelihood of moving from one state to another when an action is taken. These probabilities capture the stochastic nature of the environment in which an agent operates. These probabilities can modeled mathematically as:\n",
        "\n",
        "$$P(s'| s,a) $$\n",
        "\n",
        "where $s'$ is the next state, $s$ is the current state, and $a$ is the action taken. In a deterministic environment, the transition probability would be 1 for the next state reached by taking a particular action from a specific state, and 0 for all other states.\n",
        "\n",
        "We can represent state transitions as a state transition matrix:\n",
        "\n",
        "$$\n",
        "P = \\begin{bmatrix}\n",
        "P(s_1' | s_1, a) & P(s_2' | s_1, a) & \\dots & P(s_n' | s_1, a) \\\\\n",
        "P(s_1' | s_2, a) & P(s_2' | s_2, a) & \\dots & P(s_n' | s_2, a) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "P(s_1' | s_n, a) & P(s_2' | s_n, a) & \\dots & P(s_n' | s_n, a)\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "TUtM3bW1nmjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Transition Probabilities with Stochastic Environment (Mars Rover)\n",
        "\n",
        "Lets use our rover example to put this into action. Let us create a probability matrix from going from one state to another:\n",
        "\n",
        "$$\n",
        "P =\n",
        "\\begin{bmatrix}\n",
        "0.6 & 0.4 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "0.4 & 0.2 & 0.4 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0.4 & 0.2 & 0.4 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0.4 & 0.2 & 0.4 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0.4 & 0.2 & 0.4 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0.4 & 0.2 & 0.4 \\\\\n",
        "0 & 0 & 0 & 0 & 0 & 0.4 & 0.6 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In our transition probability matrix $P$, each row the current state the rover is in, and each column corresponds to the next possible states the rover can transition to based on the available actions. For instance, let's take the row that corresponds to the third state:\n",
        "$$\n",
        "[0 \\quad 0.4 \\quad 0.2 \\quad 0.4 \\quad 0 \\quad 0 \\quad 0]\n",
        "$$\n",
        "\n",
        "This means that if the rover is in $s_3$ and takes an action, there is a 40% chance it will move left to $s_2$, a 20% chance it will stay in $s_3$, and a 40% chance it will move right to $s_4$."
      ],
      "metadata": {
        "id": "9wglnomfvLlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## G. Expected Return\n",
        "\n",
        "The goal of the agent in a MDP is to maximize cumulative rewards over time. In order to do this, we introduce the concept of **expected return** $G_t$, which can be thought of as the sum of expected discounted future rewards. Below is the equation:\n",
        "\n",
        "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\n",
        "\n",
        "- **$\\gamma$** is the discount factor, determining the importance of future rewards relative to immediate rewards. A value closer to 1 indicates that the agent heavily values future rewards, considering them almost as important as immediate ones. A value closer to 0 means the agent heavily discounts future rewards, prioritizing immediate gains more.\n",
        "\n",
        "- $k$ represents the total amount of time steps in the future until the agent hits the terminal state, or the end of the episode.\n",
        "- **$R_{t+k+1}$** stands for the reward received at a particular time step. $R_{t+1}$ is the immediate reward after time $t$, $R_{t+2}$ is the reward one time step after $t$, and so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "MiU34uW1nrLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Example with Mars Rover\n",
        "\n",
        "Suppose the Mars rover uses the trajectory that we mentioned earlier:\n",
        "\n",
        "$$\n",
        "\\text{trajectory} = (s_4, a_3, 0, s_5, a_3, 10, s_6, a_3, 0, s_7, a_3, \\text{terminal})\n",
        "$$\n",
        "\n",
        "#### Calculating Expected Return:\n",
        "Let's compute the expected return $G_t$ for this trajectory using the given reward values and a discount factor $\\gamma = 0.9$.\n",
        "\n",
        "Given the equation for expected return:\n",
        "$$\n",
        "G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
        "$$\n",
        "\n",
        "#### Calculation:\n",
        "Using the trajectory's rewards and the discount factor:\n",
        "\n",
        "- $R_{t+1} = 0$ (Reward after taking $a_3$ from $s_4$)\n",
        "- $R_{t+2} = 10$ (Reward after taking $a_3$ from $s_5$)\n",
        "- $R_{t+3} = 0$ (Reward after taking $a_3$ from $s_6$)\n",
        "\n",
        "Substituting into the formula with $\\gamma = 0.9$:\n",
        "\n",
        "$$\n",
        "G_t = 0 + 10 \\cdot 0.9 + 0 \\cdot 0.9^2 = 0 + 9 + 0 = 9\n",
        "$$\n",
        "\n",
        "#### Interpretation:\n",
        "The expected return $G_t$ for this trajectory, considering the discount factor of $0.9$, is $9$. This value represents the cumulative expected discounted future rewards from the initial state $s_4$ to the terminal state $s_7$. This calculation shows how the concept of expected return aggregates future rewards while considering the importance of immediate vs future rewards based on the specified discount factor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lG-R0F4l3KTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## H. Policies\n",
        "\n",
        "A policy $π$ in a MDP is the strategy or behavior that an agent takes to make decisions in a environment. We use $\\pi(a|s)$ to represent the probability of taking action $a$ given that the agent is in state $s$ while under policy $\\pi$. For each state $s \\in \\textbf{S}$, $\\pi$ is the probability distribution over $a \\in \\textbf{A}$. Essentially:\n",
        "\n",
        "$$\n",
        "\\pi(a|s) = P(a_t = a \\mid s_t =s)\n",
        "$$"
      ],
      "metadata": {
        "id": "x1jrBqhWQN0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Value Functions\n",
        "\n",
        "Value functions are used to estimate the expected return an agent can achieve from being in a particular state or taking a specific action in an environment. There are two types of value functions:\n",
        "\n",
        "1) **State-value function**: Represents the expected return an agent can achieve when being in a specific state $s$ while following policy $\\pi$. Below is the state-value function:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    V^\\pi(s) &= \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right] \\\\\n",
        "    &= \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s \\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "- $V^\\pi(s)$: This is the state-value function under policy $\\pi$. It shows the expected return or cumulative sum of rewards the agent anticipates from being in state $s$ and following policy $\\pi$.\n",
        "\n",
        "- $\\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]$: This part represents the expected value $\\mathbb{E}_\\pi$ of $G_t$, which is the return at time step $t$. The condition $S_t = s$ specifies that this expectation is taken when the agent is in state $s$ at time $t$.\n",
        "\n",
        "2) **Action-value function**: Also called the Q-function, the action-value function gives us the value of the agent following an action $a$ while in state $s$ while following policy $\\pi$. Below is the action-value function:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    Q^\\pi(s, a) &= \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right] \\\\\n",
        "    &= \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a \\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "- $Q^\\pi(s, a)$: This function calculates the expected return or cumulative sum of rewards the agent anticipates from being in state $s$, taking action $a$, and following policy $\\pi$. It measures the quality of choosing action $a$ while in state $s$ under policy $\\pi$.\n",
        "\n",
        "- $\\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]$: This expression represents the expected return $\\mathbb{E}_\\pi$ from the action-value function. It considers the expected value of the return $G_t$ given that the state at time $t$ is $s$ ($S_t = s$) and the action at time $t$ is $a$ ($A_t = a$) under policy $\\pi$.\n",
        "\n",
        "We will mostly be focusing on the Q-function for later in the lesson for when we talk about Q-learning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlamPbi7Vrvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## J. Representing MDP as a Tuple\n",
        "\n",
        "MDP can be represented as a tuple $(S, A, P, R, \\gamma)$, where:\n",
        "\n",
        "- $S$ is a finite set of states.\n",
        "- $A$ is a finite set of actions.\n",
        "- $P$ is a state transition probability matrix, given by $P^a_{ss'} = P[s_{t+1} = s' \\mid s_t = s, a_t = a]$.\n",
        "- $R$ is a reward function, denoted as $R^a_s = \\mathbb{E}[R_{t+1} \\mid s_t = s, a_t = a]$.\n",
        "- $\\gamma$ is a discount factor, where $\\gamma \\in [0, 1]$.\n",
        "\n"
      ],
      "metadata": {
        "id": "K2g6_pReay6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Optimality\n",
        "\n",
        "This section will talk about how we get the optimal policy in reinforcement learning."
      ],
      "metadata": {
        "id": "jG0-HB76hwPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## A. Policy Improvement Theorem\n",
        "\n",
        "The *Policy Improvement Theorem* in reinforcement learning states that if a policy $\\pi'$ has a higher or equal value than policy $\\pi$ for every state or state-action pair, then policy $\\pi'$ is as good as or better than policy $\\pi$. Below is how to show this mathematically :\n",
        "\n",
        "$$\n",
        "\\pi' \\ge \\pi \\Leftrightarrow V^{\\pi'}(s) \\ge V^{\\pi}(s), \\forall s \\in \\mathbf{S}\n",
        "$$\n",
        "\n",
        "This inequality states policy $\\pi'$ is better than or equal to  policy $\\pi$ if and only if the value of policy $\\pi'$ is better than or equal to the value of $\\pi$ for all $s \\in \\mathbf{S}$"
      ],
      "metadata": {
        "id": "_xrjl-zmpCwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Optimal State-Value Function\n",
        "\n",
        "The value of the optimal policy for a state-value function, denoted as $V^*$, represents the maximum achievable value function among all possible policies in a given Markov Decision Process (MDP). Mathematically, the optimal state-value function $V^*(s)$ for a state $s$ in an MDP is defined as:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_{\\pi} V^\\pi(s), \\forall s \\in \\mathbf{S}\n",
        "$$\n",
        "\n",
        "This equation signifies that $V^*(s)$ denotes the maximum value function achievable from any state $s$ across all possible policies $\\pi$."
      ],
      "metadata": {
        "id": "_F9mu0-W4p7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Optimal Action-Value Function\n",
        "\n",
        "Similar to the state-value function, the optimal action-value function, or optimal Q-function, is defined as:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\max_{\\pi} Q^\\pi(s, a), \\forall s \\in \\mathbf{S}, \\forall a \\in \\mathbf{A}\n",
        "$$\n",
        "\n",
        "This equation says the optimal action-value $Q^*(s, a)$ is the maximum of the value function for any state-action pair, for all possible policies $\\pi$.\n"
      ],
      "metadata": {
        "id": "CR2qnP-SBYd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Bellman Optimality Equation for $Q^*$\n",
        "\n",
        "The optimal Q-function obeys the following Bellman optimality equation:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\mathbb{E}\\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(s', a')\\right]\n",
        "$$\n",
        "\n",
        "The optimal value for a state-action pair $(s, a)$ at time step $t$ and following the optimal policy thereafter is equal to the sum of the immediate reward $R_{t+1}$ and the maximum expected value of future rewards discounted by a factor $\\gamma$. This calculation considers the next state $s'$ resulting from the action $a$ in state $s$ and involves evaluating the maximum value over all possible actions $a'$ available from state $s'$."
      ],
      "metadata": {
        "id": "yi3HoW-Jn5HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E. Deriving Optimal Policy\n",
        "\n",
        "Using the Bellman equation for $Q^*$, we can derive the optimal policy with the following formula:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\text{argmax}_a Q^*(s,a)\n",
        "$$\n",
        "\n",
        "This equation means that for a given state $s$, the optimal policy $\\pi^*(s)$ selects the action $a$ that maximizes the optimal action-value function $Q^*(s,a)$ for that state.\n",
        "\n"
      ],
      "metadata": {
        "id": "747OG0dPU7d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning\n",
        "\n",
        "Q-Learning is based on the functionality of the Q-function. The Q-function, also known as the action-value function, measures the expected return or discounted sum of rewards obtained from a state by taking action first and following a policy after. Q-learning aims to discover an optimal policy by determining the highest achievable expected total reward across successive steps."
      ],
      "metadata": {
        "id": "wepK9xsw3sZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  A. Q-Value Table\n",
        "\n",
        "The Q-value table is the core component of Q-learning, representing the learned values of state-action pairs. Initializing this table correctly and updating its values are crucial for effective learning."
      ],
      "metadata": {
        "id": "3Ca3sxI3gHNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Initialization\n",
        "\n",
        "At the beginning, the Q-value table is typically initialized with arbitrary values or zeros for each state-action pair. The size of the table depends on the number of states and possible actions in the environment. For example, in a simple grid world with 4 possible actions (up, down, left, right) and 16 states, the initialization might look like this:\n",
        "\n",
        "| State | Action 1 | Action 2 | Action 3 | Action 4 |\n",
        "|-------|----------|----------|----------|----------|\n",
        "|   0   |    0     |    0     |    0     |    0     |\n",
        "|   1   |    0     |    0     |    0     |    0     |\n",
        "|  ...  |   ...    |   ...    |   ...    |   ...    |\n",
        "|  15   |    0     |    0     |    0     |    0     |\n",
        "\n",
        "\n",
        "We will discuss later in the course how these values are updated.\n",
        "\n"
      ],
      "metadata": {
        "id": "0tDTbVXsgLKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Exploration Vs Exploitation\n",
        "\n",
        "In reinforcement learning algorithms like Q-learning, exploration and exploitation are important concepts to understand. Below we will define the two:\n",
        "\n",
        "* **Exploration** - Exploration involves the agent trying out different actions to discover and learn about the environment.\n",
        "\n",
        "* **Exploitation** - Exploitation involves making decisions based on current knowledge to maximize the immediate reward. It essentially aims to optimize the agent's performance based on the current understanding of the environment.\n",
        "\n",
        "In Q-learning, it is important to find a good balance between the two. Initially, the agent needs to explore to gather information about the environment and build an accurate Q-value table (which represents the expected cumulative reward for each action in each state). As the agent learns more, it transitions towards exploitation to make decisions that maximize the expected reward based on its learned Q-values. We will discuss how we can use something called an *epsilon greedy strategy* to find this balance."
      ],
      "metadata": {
        "id": "EySyN_4OUgOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Epsilon Greedy Strategy\n",
        "\n",
        "We use the epsilon greedy strategy to help us find a good balance between exploration and exploitation. With this strategy, we define an exploration rate value $ϵ$ that is initially set to 1. This value is the probability our agent will explore the environment rather than exploit it. When $ϵ=1$, there is a 100% the agent will start out exploring the environment.\n",
        "\n",
        "At the start of each episode, $ϵ$ will decay by some rate that we set. This is done so the agent can begin to exploit the environment once it has explored. It can be done with this formula:\n",
        "\n",
        "$$\n",
        "\\epsilon_t = \\epsilon_{\\text{end}} + (\\epsilon_{\\text{start}} - \\epsilon_{\\text{end}}) \\cdot e^{-\\frac{t}{\\epsilon_{\\text{decay}}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $ \\epsilon_t $ is the exploration rate at episode $ t $,  \n",
        "- $ \\epsilon_{\\text{start}} $ is the initial exploration rate,  \n",
        "- $ \\epsilon_{\\text{end}} $ is the minimum exploration rate (final value),  \n",
        "- $ \\epsilon_{\\text{decay}} $ is the decay factor or the rate at which $ \\epsilon $ decreases over episodes,  \n",
        "- $ t $ is the episode number.\n",
        "\n",
        "\n",
        "To determine whether the agent will choose exploration or exploitation at each time step, we generate a random number $r$ between 0 and 1. We figure out whether we use exploration or exploitation with the following inequality:\n",
        "\n",
        "$$\n",
        "\\text{If } r > e_t, \\text{ then exploitation; else, exploration.}\n",
        "$$"
      ],
      "metadata": {
        "id": "4SgZyIjhZPpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Q-value Update with Q-Learning Algorithm\n",
        "\n",
        "During the Q-learning process, the Q-values are updated based on the agent's experiences in the environment. The update rule follows the formula:\n",
        "\n",
        "$$\n",
        "Q^{new}(s, a) \\leftarrow (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\left(r + \\gamma \\cdot \\max_{a'} Q(s', a')\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $Q(s, a)$is the Q-value for a particular state-action pair.\n",
        "- $\\alpha$ (alpha) is the learning rate, determining the weight of new information.\n",
        "- $r$ is the immediate reward received after taking action $a$ in state $s$.\n",
        "- $\\gamma$ (gamma) is the discount factor for future rewards.\n",
        "- $s'$ is the next state after taking action $a$ in state $s$.\n",
        "- $a'$ is the action that maximizes the Q-value in the next state $s'$.\n"
      ],
      "metadata": {
        "id": "RSwVXzkCgcCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Mars Rover Q-Learning Example\n",
        "\n",
        "Let's demonstrate how Q-learning works step-by-step with the Mars rover scenario you've presented. We'll go through the initialization of the Q-value table and simulate the learning process through iterations. Let us first remind ourselves of our envirnonment:\n",
        "\n",
        "**Possible States and Rewards:**\n",
        "\n",
        "\n",
        "| States         | $s_1$ | $s_2$ | $s_3$ | $s_4$ | $s_5$ | $s_6$ | $s_7$ |\n",
        "|----------------|-------|-------|-------|-------|-------|-------|-------|\n",
        "| Reward         | 0     | 1     | 0     | 0     | 0     | 10    |  0    |\n",
        "| Current State  |       |       |       | *     |       |       |       |\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "$a_1$: Move left.\n",
        "\n",
        "$a_2$: Stay in same spot.\n",
        "\n",
        "$a_3$: Move right.\n",
        "\n",
        "\n",
        "### Step 1. Q-Value Table Initialization\n",
        "\n",
        "Given the Mars rover problem with seven states and three actions, we'll initialize the Q-value table with zeros for each state-action pair:\n",
        "\n",
        "| States | Action $a_1$ | Action $a_2$ | Action $a_3$ |\n",
        "|--------|--------------|--------------|--------------|\n",
        "| $s_1$  | 0            | 0            | 0            |\n",
        "| $s_2$  | 0            | 0            | 0            |\n",
        "| $s_3$  | 0            | 0            | 0            |\n",
        "| $s_4$  | 0            | 0            | 0            |\n",
        "| $s_5$  | 0            | 0            | 0            |\n",
        "| $s_6$  | 0            | 0            | 0            |\n",
        "| $s_7$  | 0            | 0            | 0            |\n",
        "\n",
        "\n",
        "Given:\n",
        "- Current state: $s_4$\n",
        "- Learning rate: $\\alpha = 0.5$\n",
        "- Discount factor: $\\gamma = 0.9$\n",
        "\n",
        "### Step 2: Current State $s_4$\n",
        "\n",
        "The rover is currently in state $s_4$. We apply the epsilon-greedy strategy to choose an action.\n",
        "\n",
        "Assuming we select action $a_3$ (move right) based on the epsilon-greedy strategy.\n",
        "\n",
        "### Step 3: Transition and Reward\n",
        "\n",
        "The rover takes action $a_3$ and transitions to state $s_5$. From the rewards table, the immediate reward for transitioning from $s_4$ to $s_5$ with action $a_3$ is $0$.\n",
        "\n",
        "### Step 4: Q-Value Update\n",
        "\n",
        "Now, let's update the Q-value for the state-action pair $(s_4, a_3)$ using the correct Q-learning update formula:\n",
        "\n",
        "Given:\n",
        "- Current Q-value $Q(s_4, a_3) = 0$ (initialized value)\n",
        "- Immediate reward $r = 0$ (from transitioning to state $s_5$ with action $a_3$)\n",
        "- Next state $s_5$\n",
        "- Actions in $s_5$: $a_1$, $a_2$, $a_3$\n",
        "- Max Q-value in $s_5$: $\\max_{a'} Q(s_5, a') = \\max(Q(s_5, a_1), Q(s_5, a_2), Q(s_5, a_3))$\n",
        "\n",
        "Considering the rewards for transitioning from $s_5$ to other states as per the table:\n",
        "- $Q(s_5, a_1) = 0$\n",
        "- $Q(s_5, a_2) = 0$\n",
        "- $Q(s_5, a_3) = 10$\n",
        "\n",
        "Substituting into the formula:\n",
        "\n",
        "$$\n",
        "Q^{new}(s_4, a_3) \\leftarrow (1 - 0.5) \\cdot 0 + 0.5 \\cdot \\left(0 + 0.9 \\cdot \\max(0, 0, 10)\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q^{new}(s_4, a_3) \\leftarrow 0.5 \\cdot 0 + 0.5 \\cdot 9 = 4.5\n",
        "$$\n",
        "\n",
        "### Step 5: Update Q Table:\n",
        "\n",
        "We will now update the Q-table based on our Q-learning algorithm:\n",
        "\n",
        "| States | Action $a_1$ | Action $a_2$ | Action $a_3$ |\n",
        "|--------|--------------|--------------|--------------|\n",
        "| $s_1$  | 0            | 0            | 0            |\n",
        "| $s_2$  | 0            | 0            | 0            |\n",
        "| $s_3$  | 0            | 0            | 0            |\n",
        "| $s_4$  | 0            | 0            | 4.5          |\n",
        "| $s_5$  | 0            | 0            | 0            |\n",
        "| $s_6$  | 0            | 0            | 0            |\n",
        "| $s_7$  | 0            | 0            | 0            |\n",
        "\n",
        "Congrats! You have just updated your first Q-value with Q-learning."
      ],
      "metadata": {
        "id": "Dh2CauP9Z0KW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Learning\n",
        "\n",
        "When dealing with large or continuous state spaces, using a tabular Q-function becomes inefficient and impractical when trying to represent every state-action pair. The solution to this is *deep Q-learning*. Deep Q-learning uses deep neural networks to estimate the Q-value for each state-action pair in a given environment. The network will approximate the optimal Q-function. A deep neural network that approximates a Q-function is called a *deep Q-Network*, or *DQN*. Below is a great visualization from Sebastianelli et al.:\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Silvia-Ullo/publication/351884746/figure/fig1/AS:1067993664589824@1631640940747/Q-Learning-vs-Deep-Q-Learning.ppm\" alt=\"Alt text\" width=\"500\" height=\"400\" />"
      ],
      "metadata": {
        "id": "c3iKjQMQnG49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Deep Q-Networks (DQN)\n",
        "\n",
        "As mentioned before, the main goal of a DQN is to approximate the Q-function. Instead of storing Q-values, DQNs use neural networks (referred to as *policy networks* in DQNs) to generalize and estimate Q-values for different state-action pairs. This allows them to handle high-dimensional input spaces, such as raw pixel data from images, making them suitable for tasks like playing video games or controlling robotic systems. The optimal Q-function that the policy network tries to approximate satisfies the Bellman equation:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\mathbb{E}\\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(s', a')\\right]\n",
        "$$"
      ],
      "metadata": {
        "id": "KilZ4mPTzPSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Policy Network Architecture\n",
        "\n",
        "The policy network often follows a basic structure:\n",
        "\n",
        "1) **Input Layer**: Receives the state representation of the environment. This could be pixels from a game screen in the case of playing Atari games or a state vector in more abstract environments.\n",
        "\n",
        "2) **Hidden Layers**: Usually composed of convolutional layers followed by fully connected layers. Convolutional layers are used for processing raw pixel inputs in environments like Atari games, extracting features relevant to the game state. Fully connected layers help in learning more abstract and complex relationships between features.\n",
        "\n",
        "3) **Output Layer**: Produces Q-values for each action possible in the given state. The number of neurons in this layer corresponds to the number of possible actions in the environment."
      ],
      "metadata": {
        "id": "HaEfX4bvps8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Loss Calculation\n",
        "\n",
        "The loss from the network is calculated by comparing the output Q-values to the target Q-values from the right hand side of the Bellman equation."
      ],
      "metadata": {
        "id": "13Fgz5ZypFV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Update Parameters\n",
        "\n",
        "Just like any neural network, the DQN will backpropagate and update the parameters of the network with Stochastic Gradient Descent (SGD) to minimize the loss. We will do this until convergence."
      ],
      "metadata": {
        "id": "z77xcioF1zvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Experience Replay & Replay Memory\n",
        "\n",
        "*Experience Replay* is a technique used in training a DQN where we store the agent's experiences at each time step in a data set called the *replay memory*.\n"
      ],
      "metadata": {
        "id": "TsE6pvapqUEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Replay Memory as a Tuple\n",
        "\n",
        "We represent at time step $t$ the agents experience $e_t$ as a tuple:\n",
        "\n",
        "$$\n",
        "e_t = (s_t, a_t, r_{t+1}, s_{t+1})\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $s_t$ represents the state at time step $t$.\n",
        "- $a_t$ represents the action taken at time step $t$.\n",
        "- $r_{t+1}$ represents the reward received after taking action $a_t$ at time step $t$ and transitioning to the next state $s_{t+1}$ at time step $t+1$.\n",
        "- $s_{t+1}$ represents the resulting state after taking action $a_t$ at time step $t$.\n",
        "\n",
        "> Note: We store a finite number $N$ past experiences in replay memory."
      ],
      "metadata": {
        "id": "DBEdMyjqu-d7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Randomly Sampling Replay Memory\n",
        "\n",
        "During the training phase, mini-batches of experiences are randomly sampled from the replay buffer. We do this to break the correlation among consecutive samples."
      ],
      "metadata": {
        "id": "_I2dEDDQvRKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Training with Replay\n",
        "\n",
        "The sampled experiences are used to update the Q-network. By using a batch of past experiences to update the network, the learning algorithm can better generalize and learn from a broader range of situations.\n",
        "This helps in improving sample efficiency and overall learning stability.\n",
        "\n",
        "Here is basic overview of setting up training with replay memory:\n",
        "\n",
        "\n",
        "1. Initialize the replay memory dataset to capacity $N$.\n",
        "2. Initialize the network with random weights.\n",
        "3. For each episode $ i $ in the range of episodes:\n",
        "   - a. Initialize the starting state of the episode.\n",
        "   - b. For each time step $ t $ within the episode:\n",
        "     - i. Explore the environment and select a random action $ a_t $, or exploit the environment and select the greedy action for the given state that yields the highest Q-value.\n",
        "     - ii. Execute the selected action $ a_t $ in an emulator.\n",
        "     - iii. Observe the reward $ r_{t+1} $ given for this action and observe the next state $ s_{t+1} $ of the environment.\n",
        "     - iv. Store the entire experience tuple $ e_t = (s_t, a_t, r_{t+1}, s_{t+1}) $ in the replay memory.\n",
        "     - v. *Training Procedure... We will cover the actual training in the next section and expand on these steps!*\n",
        "\n"
      ],
      "metadata": {
        "id": "llc3SZcZvMMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a DQN using Policy Network\n",
        "\n",
        "In this section, we will discuss what happens during training for a DQN. As discussed in the last section, before we train the network for each time step $t$ in each episode, we store an experience into the replay memory dataset. We will continue further to discuss the actual training procedure."
      ],
      "metadata": {
        "id": "I7kN5qId108M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Training Steps\n",
        "\n",
        "We will discuss the training steps that occur within each time step $t$ within each episode after we store\n"
      ],
      "metadata": {
        "id": "AQOc87ehbvoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Sample a Random Batch from Replay Memory.\n",
        "\n",
        "We will randomly sample a batch from our replay memory dataset."
      ],
      "metadata": {
        "id": "rBmV4iBGjN1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Preprocess the State\n",
        "\n",
        "We take the state that was randomly sampled from our replay memory. For example, if we have a pixel input for images, we would want to preprocess our image to be compatible with our convolutional neural network via scaling, resizing, etc. We then will pass this preprocessed state into our network, or our *policy network*."
      ],
      "metadata": {
        "id": "RtuMBKEEjP6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Forward Propagation\n",
        "\n",
        "We forward propagate our inputs through our policy network to where it outputs an estimated Q-value for each possible action from the given input state."
      ],
      "metadata": {
        "id": "U6o2_-EdkNjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IV. Calculate Loss\n",
        "\n",
        "To compute the loss in Deep Q Learning, we evaluate the squared difference between the Q-value estimated by the neural network for the sampled action and the corresponding target Q-value. Since Deep Q-Learning doesn't directly use a Q-table like basic Q-learning, we approximate the maximum Q-value for the next state without explicit tabular representation.\n",
        "\n",
        "The estimation of the maximum Q-value for the next state $s'$, represented by:\n",
        "\n",
        "$$\\max_{a'} Q(s', a')$$\n",
        "\n",
        "is achieved by passing the next state $s'$ through the policy network (also known as the Q-network or value network). The policy network predicts Q-value estimates for all possible actions in state $s'$, enabling us to determine the action with the highest predicted Q-value without relying on an explicit Q-table.\n",
        "\n",
        "This approach allows the neural network to approximate Q-values for the next state based on its learned representations, circumventing the need for direct access to a complete Q-table for all state-action pairs. The loss is then calculated as:\n",
        "\n",
        "$$\n",
        "\\text{loss} = \\left(Q^*(s, a) - Q(s, a)\\right)^2\n",
        "$$\n",
        "\n",
        "where $Q^*(s, a)$ represents the target Q-value obtained by the Bellman equation, and $Q(s, a)$ is the estimated Q-value predicted by the neural network for the sampled action. Minimizing this loss during training improves the network's ability to approximate Q-values for better decision-making.\n"
      ],
      "metadata": {
        "id": "Dsz5GRVPjh8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V. Backpropagation & Gradient Descent\n",
        "\n",
        "Just like for any neural network, we then use run backpropagation and update our parameters $\\theta$ of our network to minimize our loss."
      ],
      "metadata": {
        "id": "CSDluyVEvKXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Full Training Loop\n",
        "\n",
        "Now to combine what we learned in the last section with training with replay and what we learned in this section, we can know formulate our full training loop:\n",
        "\n",
        "1. Initialize the replay memory dataset to capacity $N$.\n",
        "2. Initialize the network with random weights.\n",
        "3. For each episode $ i $ in the range of episodes:\n",
        "   - a. Initialize the starting state of the episode.\n",
        "   - b. For each time step $ t $ within the episode:\n",
        "     - i. Explore the environment and select a random action $ a_t $, or exploit the environment and select the greedy action for the given state that yields the highest Q-value.\n",
        "     - ii. Execute the selected action $ a_t $ in an emulator.\n",
        "     - iii. Observe the reward $ r_{t+1} $ given for this action and observe the next state $ s_{t+1} $ of the environment.\n",
        "     - iv. Store the entire experience tuple $ e_t = (s_t, a_t, r_{t+1}, s_{t+1}) $ in the replay memory.\n",
        "     - v. Sample random batch from replay memory.\n",
        "     - vi. Preprocess the state.\n",
        "     - vii. Forward propagation.\n",
        "     - viii. Loss calculation: Requires second pass through network for the next state.\n",
        "     - ix. Backpropagation & Gradient Descent: Update weights for policy network.\n"
      ],
      "metadata": {
        "id": "SVRu7rydwThH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Limitations of Standard DQNs\n",
        "\n",
        "While using a policy network is a big upgrade from basic Q-learning, there is a major limitation. In the standard Q-learning update process, the calculation of Q-values and target Q-values introduces potential issues due to the second pass to the network (*viii. Loss Calculation*). The problem is that both Q-values and target Q-values are calculated using the same weights, causing instability in the learning process. In the next section, we will introduce a solution to this problem."
      ],
      "metadata": {
        "id": "OPbovuUNgudF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target Network\n",
        "\n",
        "As mentioned before, policy networks have the problem of requiring a second pass to the network to calculate both Q-values and target Q-values, which causes instability during training. To deal with this limitation, we introduce a second network, called the *target network*. Rather than doing a second pass to the policy network to calculate the target Q-values, we instead get the target Q-values from a another network, the target network.\n",
        "\n",
        "Essentially, the only thing different from using just the policy network is instead of doing the second forward pass, we use the target network. The target network provides stable target Q-values that the policy network aims to approximate during training. With this, we obtain target Q-values for the next state."
      ],
      "metadata": {
        "id": "-5wpSYDdggXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Initialization\n",
        "\n",
        "\n",
        "The target network is a clone of the policy network, with its weights initially frozen to the original policy network's weights."
      ],
      "metadata": {
        "id": "UQpvGiNMoW3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Periodic Soft Update\n",
        "\n",
        "It is important to note that the weights in the target network do not stay fixed throughout the whole training duration. We use a technique called *soft updating*, which incorporates the Polyak Average:\n",
        "\n",
        "$$ \\theta_{\\text{target}} \\leftarrow \\tau \\cdot \\theta_{\\text{policy}} + (1 - \\tau) \\cdot \\theta_{\\text{target}} $$\n",
        "\n",
        "Where:\n",
        "- $ \\theta_{\\text{target}} $ represents the parameters of the target network.\n",
        "- $ \\theta_{\\text{policy}} $ represents the parameters of the policy network.\n",
        "- $ \\tau $ is a hyperparameter (a small value close to 0, e.g., 0.001) controlling the weight given to the policy network parameters during the update.\n",
        "\n",
        "This formula describes the soft update process where the target network parameters are updated as a weighted average of the current policy network parameters and the existing target network parameters.\n",
        "\n",
        "> *Note: We perform this soft update periodically after every $x$ time steps.*\n"
      ],
      "metadata": {
        "id": "hMazPWK0pLLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Updated Training Process\n",
        "\n",
        "We will now represent our updated training process using a target network. Changes to the original network process will be in bold font:\n",
        "\n",
        "1. Initialize the replay memory dataset to capacity $N$.\n",
        "2. Initialize the network with random weights.\n",
        "3. For each episode $ i $ in the range of episodes:\n",
        "   - a. Initialize the starting state of the episode.\n",
        "   - b. For each time step $ t $ within the episode:\n",
        "     - i. Explore the environment and select a random action $ a_t $, or exploit the environment and select the greedy action for the given state that yields the highest Q-value.\n",
        "     - ii. Execute the selected action $ a_t $ in an emulator.\n",
        "     - iii. Observe the reward $ r_{t+1} $ given for this action and observe the next state $ s_{t+1} $ of the environment.\n",
        "     - iv. Store the entire experience tuple $ e_t = (s_t, a_t, r_{t+1}, s_{t+1}) $ in the replay memory.\n",
        "     - v. Sample random batch from replay memory.\n",
        "     - vi. Preprocess the state.\n",
        "     - vii. Forward propagation.\n",
        "     - **viii. Loss calculation: Requires a pass through the target network for next state.**\n",
        "     - **ix. Backpropagation & Gradient Descent: Target network weights are updated every $x$ time steps with the policy network's weights.**\n"
      ],
      "metadata": {
        "id": "yE4em57vpuIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "At this point, you should have a good understanding on the math and theory behind Markov Decision Processes (MDPs), policies, Q-learning, and where we just finished, Deep Q-learning. In part II of this course, we will focus on implementing Deep Q-learning into a real world project in Python.\n",
        "\n",
        "> **[Click to Access Course Homepage](https://github.com/danplotkin/mastering_rl?tab=readme-ov-file#understanding-reinforcement-learning)**"
      ],
      "metadata": {
        "id": "nm5hJe3oo-DL"
      }
    }
  ]
}